import torch
import torch.nn as nn
from transformers import (
    pipeline, 
    WhisperProcessor, 
    WhisperForConditionalGeneration,
    Trainer, 
    TrainingArguments,
    AutoTokenizer,
    AutoModelForCausalLM,
    AutoModelForSeq2SeqLM
)
from diffusers import (
    StableDiffusionPipeline, 
    DPMSolverMultistepScheduler,
    StableDiffusionImg2ImgPipeline
)
import speech_recognition as sr
from gtts import gTTS
import pygame
import os
from datetime import datetime
import json
from datasets import load_dataset, Dataset
import logging
from typing import Dict, List, Optional, Union
import numpy as np
from PIL import Image, ImageDraw, ImageFont
import requests
from io import BytesIO

# ุฅุนุฏุงุฏ ุงูุชุณุฌูู
logging.basicConfig(level=logging.INFO)
logger = logging.getLogger(__name__)

class AdvancedAISystem:
    def __init__(self):
        self.device = "cuda" if torch.cuda.is_available() else "cpu"
        logger.info(f"Using device: {self.device}")
        
        self.load_models()
        self.conversation_history = []
        self.setup_audio_system()
        
    def load_models(self):
        """ุชุญููู ุฌููุน ุงูููุงุฐุฌ"""
        try:
            # ูููุฐุฌ ุงููุต ุงููุชูุฏู
            self.text_model = pipeline(
                "text-generation",
                model="microsoft/DialoGPT-large",
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                device_map="auto" if self.device == "cuda" else None
            )
            
            # ูููุฐุฌ ุฅูุดุงุก ุงูุตูุฑ
            self.image_pipe = StableDiffusionPipeline.from_pretrained(
                "runwayml/stable-diffusion-v1-5",
                torch_dtype=torch.float16 if self.device == "cuda" else torch.float32,
                safety_checker=None,
                requires_safety_checker=False
            )
            self.image_pipe.scheduler = DPMSolverMultistepScheduler.from_config(
                self.image_pipe.scheduler.config
            )
            self.image_pipe = self.image_pipe.to(self.device)
            
            # ูููุฐุฌ ุงูุชุนุฑู ุนูู ุงูููุงู
            self.whisper_processor = WhisperProcessor.from_pretrained("openai/whisper-small")
            self.whisper_model = WhisperForConditionalGeneration.from_pretrained(
                "openai/whisper-small"
            ).to(self.device)
            
            # ูููุฐุฌ ุงูุชุฑุฌูุฉ
            self.translator = pipeline(
                "translation_en_to_ar",
                model="Helsinki-NLP/opus-mt-en-ar"
            )
            
            # ูููุฐุฌ ุชุญููู ุงููุดุงุนุฑ
            self.sentiment_analyzer = pipeline(
                "sentiment-analysis",
                model="cardiffnlp/twitter-xlm-roberta-base-sentiment"
            )
            
            logger.info("All models loaded successfully!")
            
        except Exception as e:
            logger.error(f"Error loading models: {e}")
            raise

    def setup_audio_system(self):
        """ุฅุนุฏุงุฏ ูุธุงู ุงูุตูุช"""
        self.recognizer = sr.Recognizer()
        self.microphone = sr.Microphone()
        
        # ูุนุงูุฑุฉ ุถูุถุงุก ุงูุจูุฆุฉ
        with self.microphone as source:
            self.recognizer.adjust_for_ambient_noise(source)

    def generate_text(self, prompt: str, context: List[str] = None, **kwargs) -> Dict:
        """ุฅูุดุงุก ูุต ูุน ุงูุณูุงู ูุงูุฎูุงุฑุงุช ุงููุชูุฏูุฉ"""
        try:
            # ุจูุงุก ุงูุณูุงู ูู ุงูุชุงุฑูุฎ
            if context:
                full_prompt = "\n".join(context) + "\n" + prompt
            else:
                full_prompt = prompt
            
            response = self.text_model(
                full_prompt,
                max_length=kwargs.get('max_length', 150),
                num_return_sequences=kwargs.get('num_return_sequences', 1),
                temperature=kwargs.get('temperature', 0.7),
                do_sample=True,
                pad_token_id=self.text_model.tokenizer.eos_token_id,
                repetition_penalty=kwargs.get('repetition_penalty', 1.2)
            )
            
            generated_text = response[0]['generated_text']
            
            # ุชุญุฏูุซ ุชุงุฑูุฎ ุงููุญุงุฏุซุฉ
            self.conversation_history.append({
                'user': prompt,
                'ai': generated_text,
                'timestamp': datetime.now().isoformat()
            })
            
            return {
                'text': generated_text,
                'tokens': len(self.text_model.tokenizer.encode(generated_text)),
                'timestamp': datetime.now().isoformat()
            }
            
        except Exception as e:
            logger.error(f"Error in text generation: {e}")
            return {'error': str(e), 'text': ''}

    def generate_image(self, prompt: str, **kwargs) -> Dict:
        """ุฅูุดุงุก ุตูุฑุฉ ูุน ุฎูุงุฑุงุช ูุชูุฏูุฉ"""
        try:
            # ุชุญุณูู ุงู prompt ุชููุงุฆูุงู
            enhanced_prompt = self.enhance_prompt(prompt)
            
            image = self.image_pipe(
                enhanced_prompt,
                height=kwargs.get('height', 512),
                width=kwargs.get('width', 512),
                num_inference_steps=kwargs.get('steps', 20),
                guidance_scale=kwargs.get('guidance_scale', 7.5),
                negative_prompt=kwargs.get('negative_prompt', '')
            ).images[0]
            
            # ุญูุธ ุงูุตูุฑุฉ ูุน ุงุณู ูุฑูุฏ
            filename = f"generated_image_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
            image.save(filename)
            
            return {
                'filename': filename,
                'prompt': prompt,
                'enhanced_prompt': enhanced_prompt,
                'dimensions': image.size
            }
            
        except Exception as e:
            logger.error(f"Error in image generation: {e}")
            return {'error': str(e)}

    def enhance_prompt(self, prompt: str) -> str:
        """ุชุญุณูู ุงู prompt ุชููุงุฆูุงู"""
        enhancements = {
            'low_quality': ', high quality, detailed, professional',
            'portrait': ', professional lighting, sharp focus',
            'landscape': ', dramatic lighting, wide angle',
            'abstract': ', artistic, creative, unique style'
        }
        
        enhanced = prompt
        for key, enhancement in enhancements.items():
            if key in prompt.lower():
                enhanced += enhancement
                
        return enhanced + ', trending on artstation, 4k'

    def speech_to_text(self, audio_file: str = None, use_mic: bool = False) -> Dict:
       """ุชุญููู ุงูููุงู ุฅูู ูุต"""
        try:
            if use_mic:
                with self.microphone as source:
                    logger.info("Listening...")
                    audio = self.recognizer.listen(source)
                text = self.recognizer.recognize_google(audio, language="ar-AR")
            else:
                with open(audio_file, "rb") as f:
                    audio_data = f.read()
                inputs = self.whisper_processor(audio_data, return_tensors="pt")
                predicted_ids = self.whisper_model.generate(inputs.input_ids)
                text = self.whisper_processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
            
            return {'text': text, 'confidence': 0.95}
            
        except Exception as e:
            logger.error(f"Error in speech recognition: {e}")
            return {'error': str(e)}

    def text_to_speech(self, text: str, language: str = "ar") -> str:
        """ุชุญููู ุงููุต ุฅูู ููุงู"""
        try:
            tts = gTTS(text=text, lang=language, slow=False)
            filename = f"speech_{datetime.now().strftime('%Y%m%d_%H%M%S')}.mp3"
            tts.save(filename)
            
            # ุชุดุบูู ุงูุตูุช ุชููุงุฆูุงู
            pygame.mixer.init()
            pygame.mixer.music.load(filename)
            pygame.mixer.music.play()
            
            return filename
            
        except Exception as e:
            logger.error(f"Error in text-to-speech: {e}")
            return ''

    def analyze_sentiment(self, text: str) -> Dict:
        """ุชุญููู ุงููุดุงุนุฑ"""
        try:
            result = self.sentiment_analyzer(text)[0]
            return {
                'sentiment': result['label'],
                'score': result['score'],
                'text': text
            }
        except Exception as e:
            logger.error(f"Error in sentiment analysis: {e}")
            return {'error': str(e)}

    def translate_text(self, text: str, target_lang: str = "ar") -> str:
        """ุชุฑุฌูุฉ ุงููุต"""
        try:
            if target_lang == "ar":
                result = self.translator(text)[0]['translation_text']
            else:
                # ุฅุถุงูุฉ ูุชุฑุฌู ุฅุถุงูู ููุบุงุช ุงูุฃุฎุฑู
                pass
            return result
        except Exception as e:
            logger.error(f"Error in translation: {e}")
            return text

    def auto_train(self, dataset_name: str, model_type: str = "text") -> Dict:
        """ุงูุชุฏุฑูุจ ุงูุชููุงุฆู ุนูู ุจูุงูุงุช ุฌุฏูุฏุฉ"""
        try:
            dataset = load_dataset(dataset_name)
            
            training_args = TrainingArguments(
                output_dir=f'./results_{model_type}',
                num_train_epochs=3,
                per_device_train_batch_size=4,
                per_device_eval_batch_size=4,
                warmup_steps=500,
                weight_decay=0.01,
                logging_dir=f'./logs_{model_type}',
                logging_steps=10,
                evaluation_strategy="epoch",
                save_strategy="epoch",
                load_best_model_at_end=True,
            )

            if model_type == "text":
                model = self.text_model.model
            else:
                model = self.whisper_model

            trainer = Trainer(
                model=model,
                args=training_args,
                train_dataset=dataset['train'],
                eval_dataset=dataset['validation'] if 'validation' in dataset else dataset['test'],
            )

            trainer.train()
            
            return {
                'status': 'success',
                'model': model_type,
                'training_loss': trainer.state.log_history[-1]['loss'],
                'model_path': training_args.output_dir
            }
            
        except Exception as e:
            logger.error(f"Error in training: {e}")
            return {'error': str(e)}

    def create_logo(self, company_name: str, style: str = "modern") -> str:
        """ุฅูุดุงุก ุดุนุงุฑ ุจุณูุท"""
        try:
            # ุฅูุดุงุก ุตูุฑุฉ ุดุนุงุฑ ุฃุณุงุณูุฉ
            img = Image.new('RGB', (512, 512), color=(73, 109, 137))
            d = ImageDraw.Draw(img)
            
            # ุฅุถุงูุฉ ุงุณู ุงูุดุฑูุฉ
            try:
                font = ImageFont.truetype("arial.ttf", 40)
            except:
                font = ImageFont.load_default()
                
            d.text((100, 256), company_name, fill=(255, 255, 255), font=font)
            
            filename = f"logo_{company_name}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.png"
            img.save(filename)
            
            return filename
            
        except Exception as e:
            logger.error(f"Error creating logo: {e}")
            return ''

    def get_conversation_history(self) -> List[Dict]:
        """ุงูุญุตูู ุนูู ุชุงุฑูุฎ ุงููุญุงุฏุซุฉ"""
        return self.conversation_history

    def save_conversation(self, filename: str = None):
        """ุญูุธ ุงููุญุงุฏุซุฉ"""
        if not filename:
            filename = f"conversation_{datetime.now().strftime('%Y%m%d_%H%M%S')}.json"
        
        with open(filename, 'w', encoding='utf-8') as f:
            json.dump(self.conversation_history, f, ensure_ascii=False, indent=2)

# ูุซุงู ุนูู ุงูุงุณุชุฎุฏุงู
if __name__ == "__main__":
    # ุฅูุดุงุก ุงููุธุงู
    ai_system = AdvancedAISystem()
    
    # ุงุฎุชุจุงุฑ ุงููุธุงุฆู
    print("๐ ุงุฎุชุจุงุฑ ูุธุงู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ุงููุชูุฏู...")
    
    # ุฅูุดุงุก ูุต
    text_result = ai_system.generate_text(
        "ูุง ูู ูุณุชูุจู ุงูุฐูุงุก ุงูุงุตุทูุงุนู ูู ุงูุนุงูู ุงูุนุฑุจูุ",
        context=["ุฃูุช ูุณุงุนุฏ ุฐูู ูุชุฎุตุต ูู ุงูุชูููููุฌูุง."]
    )
    print("๐ ุงููุต ุงููููุฏ:", text_result['text'])
    
    # ุฅูุดุงุก ุตูุฑุฉ
    image_result = ai_system.generate_image(
        "ููุธุฑ ุทุจูุนู ุฌุจูู ูุน ุจุญูุฑุฉ ุตุงููุฉ ูู ุงูุดุฑู ุงูุฃูุณุท",
        height=512,
        width=512
    )
    print("๐ผ๏ธ ุงูุตูุฑุฉ ุงููููุฏุฉ:", image_result['filename'])
    
    # ุชุญููู ุงููุดุงุนุฑ
    sentiment = ai_system.analyze_sentiment("ุฃูุง ุณุนูุฏ ุฌุฏุงู ุจูุฐุง ุงูุชูุฏู ุงูุชูููููุฌู")
    print("๐ ุชุญููู ุงููุดุงุนุฑ:", sentiment)
    
    # ุฅูุดุงุก ุดุนุงุฑ
    logo = ai_system.create_logo("ุดุฑูุฉ ุงูุชูููุฉ", "modern")
    print("๐จ ุงูุดุนุงุฑ ุงููููุฏ:", logo)
    
    # ุญูุธ ุงููุญุงุฏุซุฉ
    ai_system.save_conversation()
    print("๐พ ุงููุญุงุฏุซุฉ ูุญููุธุฉ!")
